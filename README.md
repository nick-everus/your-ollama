ğŸ§  Local Multimodal LLM Demo (Ollama + LLaVA)

A minimal end-to-end multimodal AI demo that lets users upload an image and send a text prompt to a local LLM for vision + language reasoning.

This project runs entirely on your machine using Ollama and the LLaVA vision model â€” no cloud APIs, no keys, no cost.

â¸»

âœ¨ Features
	â€¢	ğŸ–¼ï¸ Image upload (PNG / JPG / etc.)
	â€¢	ğŸ’¬ Text prompt input
	â€¢	ğŸ§  Multimodal inference (image + text)
	â€¢	âš¡ Local LLM via Ollama
	â€¢	ğŸŒ Simple web UI
	â€¢	ğŸ”Œ Node.js + Express backend
	â€¢	ğŸ”’ No external services required

    
ğŸ“¦ Install Ollama
https://ollama.com/download
ollama pull llava

npm install ( if not already installed )

â–¶ï¸ Run the App
Start the server:
PORT=3001 node server.js
Open in your browser:
http://localhost:3001